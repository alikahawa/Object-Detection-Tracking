{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project requires AWS account!\n",
    "\n",
    "You can then create a jupyter-notebook instance (the free one works well). After that you can upload this notebook and run it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 3.0 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: tqdm\n",
      "Successfully installed tqdm-4.64.0\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pillow in /usr/lib64/python3.10/site-packages (8.3.2)\n",
      "Collecting pillow\n",
      "  Downloading Pillow-9.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 5.9 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pillow\n",
      "Successfully installed pillow-9.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tqdm\n",
    "!pip3 install pillow --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'boto3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3e3d4e749129>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mboto3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msagemaker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'boto3'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import tarfile\n",
    "import urllib\n",
    "import shutil\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from matplotlib import pyplot as plt\n",
    "from xml.etree import ElementTree as ET\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "urls_list = ['http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz',\n",
    "        'http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz']\n",
    "\n",
    "def download_and_extract(data_dir, download_dir, urls):\n",
    "    for url in urls:\n",
    "        target_file = url.split('/')[-1]\n",
    "        if target_file not in os.listdir(download_dir):\n",
    "            print('Downloading', url)\n",
    "            urllib.request.urlretrieve(url, os.path.join(download_dir, target_file))\n",
    "            tf = tarfile.open(url.split('/')[-1])\n",
    "            tf.extractall(data_dir)\n",
    "        else:\n",
    "            print('Already downloaded', url)\n",
    "\n",
    "if not os.path.isdir('data'):\n",
    "    os.mkdir('data')\n",
    "\n",
    "download_and_extract('data', '.', urls_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Annotations from XML Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_dir = 'data/annotations/xmls/'\n",
    "xml_files = [os.path.join(xml_dir, x) for x in os.listdir(xml_dir) if x[-3:] == 'xml']\n",
    "xml_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['cat', 'dog']\n",
    "categories = [\n",
    "    {\n",
    "        'class_id': 0,\n",
    "        'name': 'cat'\n",
    "    },\n",
    "    {\n",
    "        'class_id': 1,\n",
    "        'name': 'dog'\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few ways to provide data to sagemaker algorithms. \n",
    "- One of the ways is to create a record file in a format which, which the mxnet framework expects, because the sagemaker algorithms are actually built on top of mxnet.\n",
    "- A more generic and more user friendly way is providing data as JSON files to these algorithms. But the trick is that those Jason Files need to follow a very strict pattern and adhere to a strict pattern so that the record files can be created from these Jason Files before the algorithm actually starts to train.\n",
    "\n",
    "\n",
    "So at a pre processing step, we will actually generate the required data set files by reading these Jason annotation\n",
    "files. So we cannot use the X similar annotations as is. And that is the reason why we need to ah, convert the XML\n",
    "annotations to Jason annotations and the structure is pretty straightforward.\n",
    "\n",
    "What we will do is we will take a look at an XML file and then we'll find the file name from XML, and the key file in our Json annotations will be set to the categories defined before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_annotation(xml_file_path):\n",
    "    \n",
    "    tree = ET.parse(xml_file_path)\n",
    "    root = tree.getroot()\n",
    "    annotation = {}\n",
    "    \n",
    "    annotation['file'] = root.find('filename').text\n",
    "    annotation['categories'] = categories\n",
    "    \n",
    "    size = root.find('size')\n",
    "    \n",
    "    annotation['image_size'] = [{\n",
    "        'width': int(size.find('width').text),\n",
    "        'height': int(size.find('height').text),\n",
    "        'depth': int(size.find('depth').text)\n",
    "    }]\n",
    "    \n",
    "    annotation['annotations'] = []\n",
    "    \n",
    "    for item in root.iter('object'):\n",
    "        class_id = classes.index(item.find('name').text)\n",
    "        ymin, xmin, ymax, xmax = None, None, None, None\n",
    "        \n",
    "        for box in item.findall('bndbox'):\n",
    "            xmin = int(box.find(\"xmin\").text)\n",
    "            ymin = int(box.find(\"ymin\").text)\n",
    "            xmax = int(box.find(\"xmax\").text)\n",
    "            ymax = int(box.find(\"ymax\").text)\n",
    "        \n",
    "            if all([xmin, ymin, xmax, ymax]) is not None:\n",
    "                 annotation['annotations'].append({\n",
    "                     'class_id': class_id,\n",
    "                     'left': xmin,\n",
    "                     'top': ymin,\n",
    "                     'width': xmax - xmin,\n",
    "                     'height': ymax - ymin\n",
    "                 })\n",
    "    return annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_annotation(xml_files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_example(plt, annot, image_file_path, rows, cols, index):\n",
    "    img = Image.open(image_file_path)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    font = ImageFont.truetype('/usr/share/fonts/dejavu/DejaVuSerif-Bold.ttf', 20)\n",
    "    \n",
    "    for a in annot['annotations']:\n",
    "        box = [\n",
    "            int(a['left']), int(a['top']),\n",
    "            int(a['left']) + int(a['width']),\n",
    "            int(a['top']) + int(a['height'])\n",
    "        ]\n",
    "        draw.rectangle(box, outline='yellow', width=4)\n",
    "        draw.text((box[0], box[1]), classes[int(a['class_id'])], font=font)\n",
    "    plt.subplot(rows, cols, index + 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(img)\n",
    "    return plt\n",
    "\n",
    "def show_random_annotations(plt):\n",
    "    plt.figure(figsize=(16, 16))\n",
    "    for i in range(0, 9):\n",
    "        index = random.randint(0, len(xml_files) - 1)\n",
    "        \n",
    "        annot = extract_annotation(xml_files[index])\n",
    "        image_file_path = os.path.join('data/images/', annot['file'])\n",
    "\n",
    "        plt = plot_example(plt, annot, image_file_path, 3, 3, i)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random_annotations(plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Setup\n",
    "\n",
    "First we create the role, which bis needed when we create a sagemaker estimator, we will also need\n",
    "a bucket on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "bucket_name = 'petsdata'\n",
    "\n",
    "training_image = get_image_uri(boto3.Session().region_name(), 'object-detection', repo_version = 'latest')\n",
    "\n",
    "print(training_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = ['train', 'train_annotation', 'validation', 'validation_annotation']\n",
    "\n",
    "for folder in folders:\n",
    "    if os.path.isdir(folder):\n",
    "        shutil.rmtree(folder)\n",
    "    os.mkdir(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Data for SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total examples:', len(xml_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for xml_file in tqdm(xml_files):\n",
    "    target_set = 'train' if random.randint(0, 99) < 75 else 'validation'\n",
    "    annot = extract_annotation(xml_file)\n",
    "    image_file_path = os.path.join('data/images/', annot['file'])\n",
    "    image_target_path = os.path.join(target_set, annot['file'])\n",
    "    shutil.copy(image_file_path, image_target_path)\n",
    "    json_file_path = os.path.join(target_set + '_annotation', annot['file'][:-3] + 'json')\n",
    "    with open(json_file_path, 'w') as f:\n",
    "        json.dump(annot, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = os.listdir('train')\n",
    "train_annots = os.listdir('train_annotation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_annots), len(train_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a check to check if all the file names to matchup.\n",
    "for image in train_images:\n",
    "    key = image.split('.')[0]\n",
    "    json_file = key + '.json'\n",
    "    if json_file not in train_annots:\n",
    "        print('Not found', json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading Data to S3\n",
    "This process takes a bit of time, up to 10 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we start a session\n",
    "sees = sagemaker.Session()\n",
    "\n",
    "# We upload the images to S3 to train it\n",
    "print('Uploading data ..') # This is check point\n",
    "s3_train_path = sess.upload_data(path='train', bucket= bucket_name, key_perfix= 'train')\n",
    "print('Data is uploaded ..')\n",
    "s3_validation_path = sess.upload_data(path='validation', bucket= bucket_name, key_perfix= 'validation')\n",
    "\n",
    "# Upload the annotation data\n",
    "print('Uploading annotation data ..')\n",
    "s3_train_annotations = sess.upload_data(path='train_annotations', bucket= bucket_name, \n",
    "                                        key_perfix= 'train_annotations')\n",
    "print('Annotation data is uploaded ..')\n",
    "s3_validation_annotations = sess.upload_data(path='validation_annotations', bucket= bucket_name, \n",
    "                                             key_perfix= 'validation_annotations')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_validation_annotations # This will return where the validation annotation is located"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Estimator\n",
    "Now let's create a safe to make an estimator, which is\n",
    "basically a high level API which will handle the training job\n",
    "for us.\n",
    "\n",
    "- So we need to specify the training image and set an execution role.\n",
    "- For training, we use an instance type of ml dot Petri 0.0.2 x Large, this instance has 16 GB of GPU memory. We will use only one instance.\n",
    "\n",
    "- Next set the training volume size 100 TB\n",
    "- Set a time limit for this training job 36,000 seconds.\n",
    "- When you use the decent annotations like we have, you want to use the file mode.\n",
    "- The output path is the path where you will store the train model artifact and let's store it in the in a in a new full record output in pets data bucket \n",
    "- finally, we will specify sagemaker session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sagemaker.estimator.Estimator(\n",
    "    training_image,\n",
    "    role = role,\n",
    "    train_instance_type ='ml.p3.2xlarge',\n",
    "    train_instance_count = 1,\n",
    "    train_volume_size = 100,\n",
    "    train_max_rum = 3600,\n",
    "    input_mode = 'File',\n",
    "    output_path = 's3://petsdata/output',\n",
    "    sagemaker_session = sess\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters\n",
    "Which are relevant with respect to the object\n",
    "detection SSD algorithm that we are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_hyperparameters(\n",
    "    base_network = 'resnet-50',\n",
    "    num_classes = 2,\n",
    "    use_pretrained_model = 1,\n",
    "    mini_batch_size = 16,\n",
    "    epochs = 15,\n",
    "    learning_rate = 0.001,\n",
    "    optimizer = 'sgd',\n",
    "    lr_scheduler_step = '10',\n",
    "    le_scheduler_factor = 0.1,\n",
    "    momentum = 0.9,\n",
    "    weight_decay = 0.0005,\n",
    "    overlap_threshold = 0.45,\n",
    "    image_shape = 512,\n",
    "    num_training_samples = len(train_annots)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.session import s3_input\n",
    "train_data = s3_input(s3_train_path,\n",
    "                       distribution = 'FullyReplicated',\n",
    "                       content_type = 'application/x-image',\n",
    "                       s3_data_type = 'S3Prefix')\n",
    "\n",
    "validation_data = s3_input(s3_validation_path,\n",
    "                       distribution = 'FullyReplicated',\n",
    "                       content_type = 'application/x-image',\n",
    "                       s3_data_type = 'S3Prefix')\n",
    "\n",
    "train_annotation_data = s3_input(s3_train_annotations,\n",
    "                       distribution = 'FullyReplicated',\n",
    "                       content_type = 'application/x-image',\n",
    "                       s3_data_type = 'S3Prefix')\n",
    "\n",
    "validation_annotations_data = s3_input(s3_validation_annotations,\n",
    "                       distribution = 'FullyReplicated',\n",
    "                       content_type = 'application/x-image',\n",
    "                       s3_data_type = 'S3Prefix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_channels = {\n",
    "    'train': train_Data,\n",
    "    'validation': validation_data,\n",
    "    'train_annotation': train_annotation_data,\n",
    "    'validation_annotation': validation_annotations_data\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(inputs = data_channel, logs = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Model\n",
    "A sagemaker is going to spin\n",
    "up an instance of this type, and then it will copy the model\n",
    "artifact to this instance and ready for influence\n",
    "and basically start serving it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployed_model = model.deploy(initial_instance_count = 1, instance_type = 'ml.m4.xlarge')\n",
    "print('\\nModeldeployed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = 'validation'\n",
    "images = [x for x in os.listdir(image_dir) if x[-3:] == 'jpg']\n",
    "print(len(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployed_model.content_type = 'image/jpeg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "\n",
    "image_path = os.path.join(image_dir, images[index])\n",
    "# image_path = 'dog_cat.jfif'\n",
    "\n",
    "with open(image_path, 'rb') as f:\n",
    "    b = bytearray(f.read())\n",
    "\n",
    "results = deployed_model.predict(b)\n",
    "results = json.loads(results)\n",
    "\n",
    "preds = results['prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(image_path)\n",
    "draw = ImageDraw.Draw(img)\n",
    "font = ImageFont.truetype('/usr/share/fonts/dejavu/DejaVuSerif-Bold.ttf', 30)\n",
    "w, h =img.size\n",
    "\n",
    "for pred in preds:\n",
    "    # extract info out of the predictions\n",
    "    class_id, score, xmin, ymin, xmax, ymax = pred\n",
    "    \n",
    "    if score > 0.7:\n",
    "        box = [w*xmin, h*ymin, w*xmax, h*ymax]\n",
    "\n",
    "        draw.rectangle(box, outline='yellow', width=4)\n",
    "        draw.text((box[0], box[1]), classes[int(class_id)], font=font, fill='#000000')\n",
    "    else:\n",
    "        break\n",
    "\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget!! You need to delete endpoint or else you will continue to accrue cost!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(deployed_model.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
